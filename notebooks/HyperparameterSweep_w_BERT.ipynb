{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "# Get all script paths\n",
    "import sys\n",
    "sys.path.append('..') \n",
    "\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import *\n",
    "import wandb\n",
    "from pipeline import train_complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: xkytzga7\n",
      "Sweep URL: https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 46s7j354 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mski\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_192618-46s7j354</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/46s7j354\" target=\"_blank\">bright-sweep-1</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.45795086191855106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.3800171227203853\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.339 MB of 0.340 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.996561…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>loss_batch</td><td>██▆▄▃▃▄▃▄▃▃▃▃▃▂▃▂▂▂▃▂▃▂▃▂▂▂▂▁▁▂▁▂▂▂▁▁▂▂▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_acc_0</td><td>▁</td></tr><tr><td>val_acc_1</td><td>▁</td></tr><tr><td>val_acc_2</td><td>▁</td></tr><tr><td>val_acc_3</td><td>▁</td></tr><tr><td>val_acc_4</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_f1_0</td><td>▁</td></tr><tr><td>val_f1_1</td><td>▁</td></tr><tr><td>val_f1_2</td><td>▁</td></tr><tr><td>val_f1_3</td><td>▁</td></tr><tr><td>val_f1_4</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_loss_batch</td><td>▁</td></tr><tr><td>val_prec</td><td>▁</td></tr><tr><td>val_rec</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.45795</td></tr><tr><td>loss_batch</td><td>0.08366</td></tr><tr><td>val_acc</td><td>0.86971</td></tr><tr><td>val_acc_0</td><td>0.85515</td></tr><tr><td>val_acc_1</td><td>0.87184</td></tr><tr><td>val_acc_2</td><td>0.86491</td></tr><tr><td>val_acc_3</td><td>0.87309</td></tr><tr><td>val_acc_4</td><td>0.87655</td></tr><tr><td>val_f1</td><td>0.87252</td></tr><tr><td>val_f1_0</td><td>0.78386</td></tr><tr><td>val_f1_1</td><td>0.91525</td></tr><tr><td>val_f1_2</td><td>0.80912</td></tr><tr><td>val_f1_3</td><td>0.83149</td></tr><tr><td>val_f1_4</td><td>0.86715</td></tr><tr><td>val_loss</td><td>0.38002</td></tr><tr><td>val_loss_batch</td><td>0.0851</td></tr><tr><td>val_prec</td><td>0.88039</td></tr><tr><td>val_rec</td><td>0.86971</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">bright-sweep-1</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/46s7j354\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/46s7j354</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_192618-46s7j354/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zdeqkyh6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_194444-zdeqkyh6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/zdeqkyh6\" target=\"_blank\">rose-sweep-2</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.48928117435416024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.47632692560037626\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.22999256632597423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.3858030080613197\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.688 MB of 0.689 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.998303…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▁</td></tr><tr><td>loss_batch</td><td>█▇▄▃▄▃▃▂▂▂▂▁▂▃▂▁▃▂▁▂▁▁▁▂▂▂▁▂▂▂▁▂▁▂▃▂▂▁▂▂</td></tr><tr><td>val_acc</td><td>▁█</td></tr><tr><td>val_acc_0</td><td>█▁</td></tr><tr><td>val_acc_1</td><td>▁█</td></tr><tr><td>val_acc_2</td><td>█▁</td></tr><tr><td>val_acc_3</td><td>▁█</td></tr><tr><td>val_acc_4</td><td>█▁</td></tr><tr><td>val_f1</td><td>▁█</td></tr><tr><td>val_f1_0</td><td>▁█</td></tr><tr><td>val_f1_1</td><td>▁█</td></tr><tr><td>val_f1_2</td><td>▁█</td></tr><tr><td>val_f1_3</td><td>▁█</td></tr><tr><td>val_f1_4</td><td>▁█</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_loss_batch</td><td>█▁</td></tr><tr><td>val_prec</td><td>▁█</td></tr><tr><td>val_rec</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.22999</td></tr><tr><td>loss_batch</td><td>0.05843</td></tr><tr><td>val_acc</td><td>0.87609</td></tr><tr><td>val_acc_0</td><td>0.84865</td></tr><tr><td>val_acc_1</td><td>0.88592</td></tr><tr><td>val_acc_2</td><td>0.85417</td></tr><tr><td>val_acc_3</td><td>0.88204</td></tr><tr><td>val_acc_4</td><td>0.87384</td></tr><tr><td>val_f1</td><td>0.87834</td></tr><tr><td>val_f1_0</td><td>0.79026</td></tr><tr><td>val_f1_1</td><td>0.92168</td></tr><tr><td>val_f1_2</td><td>0.82119</td></tr><tr><td>val_f1_3</td><td>0.83219</td></tr><tr><td>val_f1_4</td><td>0.86447</td></tr><tr><td>val_loss</td><td>0.3858</td></tr><tr><td>val_loss_batch</td><td>0.0715</td></tr><tr><td>val_prec</td><td>0.88401</td></tr><tr><td>val_rec</td><td>0.87609</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rose-sweep-2</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/zdeqkyh6\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/zdeqkyh6</a><br/>Synced 6 W&B file(s), 6 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_194444-zdeqkyh6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s16rkxpr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_202122-s16rkxpr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/s16rkxpr\" target=\"_blank\">avid-sweep-3</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3353185185314812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.26436667208030606\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.250 MB of 0.251 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.995359…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>loss_batch</td><td>█▇▄▄▃▂▃▂▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▃▂▂▁▂▁▁▁▂▂▂▂▁▁▁▁▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_acc_0</td><td>▁</td></tr><tr><td>val_acc_1</td><td>▁</td></tr><tr><td>val_acc_2</td><td>▁</td></tr><tr><td>val_acc_3</td><td>▁</td></tr><tr><td>val_acc_4</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_f1_0</td><td>▁</td></tr><tr><td>val_f1_1</td><td>▁</td></tr><tr><td>val_f1_2</td><td>▁</td></tr><tr><td>val_f1_3</td><td>▁</td></tr><tr><td>val_f1_4</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_loss_batch</td><td>▁</td></tr><tr><td>val_prec</td><td>▁</td></tr><tr><td>val_rec</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.33532</td></tr><tr><td>loss_batch</td><td>0.0671</td></tr><tr><td>val_acc</td><td>0.90977</td></tr><tr><td>val_acc_0</td><td>0.89315</td></tr><tr><td>val_acc_1</td><td>0.91192</td></tr><tr><td>val_acc_2</td><td>0.90958</td></tr><tr><td>val_acc_3</td><td>0.90521</td></tr><tr><td>val_acc_4</td><td>0.92221</td></tr><tr><td>val_f1</td><td>0.91106</td></tr><tr><td>val_f1_0</td><td>0.84537</td></tr><tr><td>val_f1_1</td><td>0.94144</td></tr><tr><td>val_f1_2</td><td>0.87392</td></tr><tr><td>val_f1_3</td><td>0.87192</td></tr><tr><td>val_f1_4</td><td>0.91076</td></tr><tr><td>val_loss</td><td>0.26437</td></tr><tr><td>val_loss_batch</td><td>0.03399</td></tr><tr><td>val_prec</td><td>0.91463</td></tr><tr><td>val_rec</td><td>0.90977</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">avid-sweep-3</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/s16rkxpr\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/s16rkxpr</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_202122-s16rkxpr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mk70mka9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_205424-mk70mka9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/mk70mka9\" target=\"_blank\">flowing-sweep-4</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4032615976404327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.5030696640007365\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.22444122251328252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.40542360695644214\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.691 MB of 0.692 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.998310…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▁</td></tr><tr><td>loss_batch</td><td>▆█▄▄▆▄▅▃▄▂▄▂▄▄▂▂▃▂▁▂▂▂▁▃▃▂▂▄▂▂▁▂▂▂▄▂▂▂▂▃</td></tr><tr><td>val_acc</td><td>▁█</td></tr><tr><td>val_acc_0</td><td>█▁</td></tr><tr><td>val_acc_1</td><td>▁█</td></tr><tr><td>val_acc_2</td><td>▁█</td></tr><tr><td>val_acc_3</td><td>█▁</td></tr><tr><td>val_acc_4</td><td>█▁</td></tr><tr><td>val_f1</td><td>▁█</td></tr><tr><td>val_f1_0</td><td>▁█</td></tr><tr><td>val_f1_1</td><td>▁█</td></tr><tr><td>val_f1_2</td><td>▁█</td></tr><tr><td>val_f1_3</td><td>▁█</td></tr><tr><td>val_f1_4</td><td>▁█</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_loss_batch</td><td>█▁</td></tr><tr><td>val_prec</td><td>▁█</td></tr><tr><td>val_rec</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.22444</td></tr><tr><td>loss_batch</td><td>0.05103</td></tr><tr><td>val_acc</td><td>0.87692</td></tr><tr><td>val_acc_0</td><td>0.8454</td></tr><tr><td>val_acc_1</td><td>0.88829</td></tr><tr><td>val_acc_2</td><td>0.85481</td></tr><tr><td>val_acc_3</td><td>0.87572</td></tr><tr><td>val_acc_4</td><td>0.88003</td></tr><tr><td>val_f1</td><td>0.87906</td></tr><tr><td>val_f1_0</td><td>0.79275</td></tr><tr><td>val_f1_1</td><td>0.92292</td></tr><tr><td>val_f1_2</td><td>0.82155</td></tr><tr><td>val_f1_3</td><td>0.83171</td></tr><tr><td>val_f1_4</td><td>0.86202</td></tr><tr><td>val_loss</td><td>0.40542</td></tr><tr><td>val_loss_batch</td><td>0.07743</td></tr><tr><td>val_prec</td><td>0.8843</td></tr><tr><td>val_rec</td><td>0.87692</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">flowing-sweep-4</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/mk70mka9\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/mk70mka9</a><br/>Synced 6 W&B file(s), 6 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_205424-mk70mka9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9my5msdi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_213146-9my5msdi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/9my5msdi\" target=\"_blank\">ruby-sweep-5</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4016699563449858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.5172265836638977\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2240954975321771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.4116134006082405\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.701 MB of 0.702 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.998334…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▁</td></tr><tr><td>loss_batch</td><td>▆█▄▄▆▄▄▃▄▂▄▂▃▅▂▂▄▃▁▂▂▂▁▃▃▂▂▄▂▂▁▂▂▂▄▂▂▃▂▃</td></tr><tr><td>val_acc</td><td>▁█</td></tr><tr><td>val_acc_0</td><td>█▁</td></tr><tr><td>val_acc_1</td><td>▁█</td></tr><tr><td>val_acc_2</td><td>▁▁</td></tr><tr><td>val_acc_3</td><td>█▁</td></tr><tr><td>val_acc_4</td><td>▁█</td></tr><tr><td>val_f1</td><td>▁█</td></tr><tr><td>val_f1_0</td><td>▁█</td></tr><tr><td>val_f1_1</td><td>▁█</td></tr><tr><td>val_f1_2</td><td>▁█</td></tr><tr><td>val_f1_3</td><td>▁█</td></tr><tr><td>val_f1_4</td><td>▁█</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_loss_batch</td><td>█▁</td></tr><tr><td>val_prec</td><td>▁█</td></tr><tr><td>val_rec</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.2241</td></tr><tr><td>loss_batch</td><td>0.04642</td></tr><tr><td>val_acc</td><td>0.87373</td></tr><tr><td>val_acc_0</td><td>0.83858</td></tr><tr><td>val_acc_1</td><td>0.88406</td></tr><tr><td>val_acc_2</td><td>0.85374</td></tr><tr><td>val_acc_3</td><td>0.87493</td></tr><tr><td>val_acc_4</td><td>0.8808</td></tr><tr><td>val_f1</td><td>0.87614</td></tr><tr><td>val_f1_0</td><td>0.78195</td></tr><tr><td>val_f1_1</td><td>0.92136</td></tr><tr><td>val_f1_2</td><td>0.81959</td></tr><tr><td>val_f1_3</td><td>0.8259</td></tr><tr><td>val_f1_4</td><td>0.86196</td></tr><tr><td>val_loss</td><td>0.41161</td></tr><tr><td>val_loss_batch</td><td>0.07562</td></tr><tr><td>val_prec</td><td>0.88215</td></tr><tr><td>val_rec</td><td>0.87373</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ruby-sweep-5</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/9my5msdi\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/9my5msdi</a><br/>Synced 6 W&B file(s), 6 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_213146-9my5msdi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wwzlsogd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_220920-wwzlsogd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/wwzlsogd\" target=\"_blank\">laced-sweep-6</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.48727389741176313\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.3930724498999823\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.341 MB of 0.342 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.996580…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>loss_batch</td><td>██▇▄▄▂▃▃▄▃▂▃▃▃▂▂▂▂▂▄▂▃▂▃▂▂▂▃▁▁▂▂▂▁▁▁▁▂▁▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_acc_0</td><td>▁</td></tr><tr><td>val_acc_1</td><td>▁</td></tr><tr><td>val_acc_2</td><td>▁</td></tr><tr><td>val_acc_3</td><td>▁</td></tr><tr><td>val_acc_4</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_f1_0</td><td>▁</td></tr><tr><td>val_f1_1</td><td>▁</td></tr><tr><td>val_f1_2</td><td>▁</td></tr><tr><td>val_f1_3</td><td>▁</td></tr><tr><td>val_f1_4</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_loss_batch</td><td>▁</td></tr><tr><td>val_prec</td><td>▁</td></tr><tr><td>val_rec</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.48727</td></tr><tr><td>loss_batch</td><td>0.08339</td></tr><tr><td>val_acc</td><td>0.86662</td></tr><tr><td>val_acc_0</td><td>0.84963</td></tr><tr><td>val_acc_1</td><td>0.86634</td></tr><tr><td>val_acc_2</td><td>0.85395</td></tr><tr><td>val_acc_3</td><td>0.8852</td></tr><tr><td>val_acc_4</td><td>0.88429</td></tr><tr><td>val_f1</td><td>0.86962</td></tr><tr><td>val_f1_0</td><td>0.7823</td></tr><tr><td>val_f1_1</td><td>0.91339</td></tr><tr><td>val_f1_2</td><td>0.80437</td></tr><tr><td>val_f1_3</td><td>0.82341</td></tr><tr><td>val_f1_4</td><td>0.86668</td></tr><tr><td>val_loss</td><td>0.39307</td></tr><tr><td>val_loss_batch</td><td>0.08546</td></tr><tr><td>val_prec</td><td>0.87841</td></tr><tr><td>val_rec</td><td>0.86662</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">laced-sweep-6</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/wwzlsogd\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/wwzlsogd</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_220920-wwzlsogd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xduer15k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_222823-xduer15k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/xduer15k\" target=\"_blank\">fragrant-sweep-7</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2749213280244545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.2585193175526598\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.258 MB of 0.259 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.995497…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>loss_batch</td><td>█▅▅▃▃▃▃▂▃▂▂▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▂▁▁▂▂▂▂▁▁▁▁▁▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_acc_0</td><td>▁</td></tr><tr><td>val_acc_1</td><td>▁</td></tr><tr><td>val_acc_2</td><td>▁</td></tr><tr><td>val_acc_3</td><td>▁</td></tr><tr><td>val_acc_4</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_f1_0</td><td>▁</td></tr><tr><td>val_f1_1</td><td>▁</td></tr><tr><td>val_f1_2</td><td>▁</td></tr><tr><td>val_f1_3</td><td>▁</td></tr><tr><td>val_f1_4</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_loss_batch</td><td>▁</td></tr><tr><td>val_prec</td><td>▁</td></tr><tr><td>val_rec</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.27492</td></tr><tr><td>loss_batch</td><td>0.05435</td></tr><tr><td>val_acc</td><td>0.91193</td></tr><tr><td>val_acc_0</td><td>0.8938</td></tr><tr><td>val_acc_1</td><td>0.91794</td></tr><tr><td>val_acc_2</td><td>0.907</td></tr><tr><td>val_acc_3</td><td>0.90021</td></tr><tr><td>val_acc_4</td><td>0.9195</td></tr><tr><td>val_f1</td><td>0.91308</td></tr><tr><td>val_f1_0</td><td>0.85004</td></tr><tr><td>val_f1_1</td><td>0.94368</td></tr><tr><td>val_f1_2</td><td>0.87906</td></tr><tr><td>val_f1_3</td><td>0.86755</td></tr><tr><td>val_f1_4</td><td>0.91192</td></tr><tr><td>val_loss</td><td>0.25852</td></tr><tr><td>val_loss_batch</td><td>0.02937</td></tr><tr><td>val_prec</td><td>0.91598</td></tr><tr><td>val_rec</td><td>0.91193</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fragrant-sweep-7</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/xduer15k\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/xduer15k</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_222823-xduer15k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d0sd86rh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_230119-d0sd86rh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/d0sd86rh\" target=\"_blank\">hardy-sweep-8</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.38501337235774485\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.39066075486588137\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.340 MB of 0.341 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.996574…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>loss_batch</td><td>█▆▆▃▃▂▄▃▅▂▂▄▄▃▂▃▃▃▃▄▃▃▃▃▃▃▂▃▁▁▂▂▂▂▂▂▁▂▂▃</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_acc_0</td><td>▁</td></tr><tr><td>val_acc_1</td><td>▁</td></tr><tr><td>val_acc_2</td><td>▁</td></tr><tr><td>val_acc_3</td><td>▁</td></tr><tr><td>val_acc_4</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_f1_0</td><td>▁</td></tr><tr><td>val_f1_1</td><td>▁</td></tr><tr><td>val_f1_2</td><td>▁</td></tr><tr><td>val_f1_3</td><td>▁</td></tr><tr><td>val_f1_4</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_loss_batch</td><td>▁</td></tr><tr><td>val_prec</td><td>▁</td></tr><tr><td>val_rec</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.38501</td></tr><tr><td>loss_batch</td><td>0.10011</td></tr><tr><td>val_acc</td><td>0.86793</td></tr><tr><td>val_acc_0</td><td>0.8519</td></tr><tr><td>val_acc_1</td><td>0.86825</td></tr><tr><td>val_acc_2</td><td>0.8576</td></tr><tr><td>val_acc_3</td><td>0.88441</td></tr><tr><td>val_acc_4</td><td>0.87926</td></tr><tr><td>val_f1</td><td>0.87087</td></tr><tr><td>val_f1_0</td><td>0.78147</td></tr><tr><td>val_f1_1</td><td>0.91426</td></tr><tr><td>val_f1_2</td><td>0.8092</td></tr><tr><td>val_f1_3</td><td>0.82581</td></tr><tr><td>val_f1_4</td><td>0.8647</td></tr><tr><td>val_loss</td><td>0.39066</td></tr><tr><td>val_loss_batch</td><td>0.09169</td></tr><tr><td>val_prec</td><td>0.87938</td></tr><tr><td>val_rec</td><td>0.86793</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hardy-sweep-8</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/d0sd86rh\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/d0sd86rh</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_230119-d0sd86rh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hmov736c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_232011-hmov736c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/hmov736c\" target=\"_blank\">different-sweep-9</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.45511314458860264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.3793251071800525\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.341 MB of 0.342 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.996580…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>loss_batch</td><td>██▅▄▃▃▄▃▄▃▃▃▃▃▂▃▂▂▂▃▃▂▂▃▂▂▂▃▁▁▂▂▂▂▂▂▁▂▁▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_acc_0</td><td>▁</td></tr><tr><td>val_acc_1</td><td>▁</td></tr><tr><td>val_acc_2</td><td>▁</td></tr><tr><td>val_acc_3</td><td>▁</td></tr><tr><td>val_acc_4</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_f1_0</td><td>▁</td></tr><tr><td>val_f1_1</td><td>▁</td></tr><tr><td>val_f1_2</td><td>▁</td></tr><tr><td>val_f1_3</td><td>▁</td></tr><tr><td>val_f1_4</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_loss_batch</td><td>▁</td></tr><tr><td>val_prec</td><td>▁</td></tr><tr><td>val_rec</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.45511</td></tr><tr><td>loss_batch</td><td>0.07938</td></tr><tr><td>val_acc</td><td>0.87061</td></tr><tr><td>val_acc_0</td><td>0.8545</td></tr><tr><td>val_acc_1</td><td>0.87237</td></tr><tr><td>val_acc_2</td><td>0.86104</td></tr><tr><td>val_acc_3</td><td>0.87625</td></tr><tr><td>val_acc_4</td><td>0.887</td></tr><tr><td>val_f1</td><td>0.87333</td></tr><tr><td>val_f1_0</td><td>0.78902</td></tr><tr><td>val_f1_1</td><td>0.91608</td></tr><tr><td>val_f1_2</td><td>0.81023</td></tr><tr><td>val_f1_3</td><td>0.82858</td></tr><tr><td>val_f1_4</td><td>0.86752</td></tr><tr><td>val_loss</td><td>0.37933</td></tr><tr><td>val_loss_batch</td><td>0.09139</td></tr><tr><td>val_prec</td><td>0.881</td></tr><tr><td>val_rec</td><td>0.87061</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">different-sweep-9</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/hmov736c\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/hmov736c</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_232011-hmov736c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sj95ysk8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220518_233831-sj95ysk8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/sj95ysk8\" target=\"_blank\">crisp-sweep-10</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3790407630260825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.27380462644086834\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.251 MB of 0.252 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.995382…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>loss_batch</td><td>█▇▆▄▃▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▁▂▁▁▁▂▂▂▁▁▁▁▁▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_acc_0</td><td>▁</td></tr><tr><td>val_acc_1</td><td>▁</td></tr><tr><td>val_acc_2</td><td>▁</td></tr><tr><td>val_acc_3</td><td>▁</td></tr><tr><td>val_acc_4</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_f1_0</td><td>▁</td></tr><tr><td>val_f1_1</td><td>▁</td></tr><tr><td>val_f1_2</td><td>▁</td></tr><tr><td>val_f1_3</td><td>▁</td></tr><tr><td>val_f1_4</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_loss_batch</td><td>▁</td></tr><tr><td>val_prec</td><td>▁</td></tr><tr><td>val_rec</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.37904</td></tr><tr><td>loss_batch</td><td>0.07007</td></tr><tr><td>val_acc</td><td>0.91008</td></tr><tr><td>val_acc_0</td><td>0.89575</td></tr><tr><td>val_acc_1</td><td>0.91076</td></tr><tr><td>val_acc_2</td><td>0.90722</td></tr><tr><td>val_acc_3</td><td>0.91285</td></tr><tr><td>val_acc_4</td><td>0.92376</td></tr><tr><td>val_f1</td><td>0.91144</td></tr><tr><td>val_f1_0</td><td>0.84446</td></tr><tr><td>val_f1_1</td><td>0.94148</td></tr><tr><td>val_f1_2</td><td>0.87635</td></tr><tr><td>val_f1_3</td><td>0.87143</td></tr><tr><td>val_f1_4</td><td>0.91246</td></tr><tr><td>val_loss</td><td>0.2738</td></tr><tr><td>val_loss_batch</td><td>0.03153</td></tr><tr><td>val_prec</td><td>0.91527</td></tr><tr><td>val_rec</td><td>0.91008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">crisp-sweep-10</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/sj95ysk8\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/sj95ysk8</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220518_233831-sj95ysk8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ebdyle2n with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220519_001204-ebdyle2n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/ebdyle2n\" target=\"_blank\">crimson-sweep-11</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.40209417273532544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.45147190010838983\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.22156387368961408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.38650380722920424\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.694 MB of 0.696 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.998318…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▁</td></tr><tr><td>loss_batch</td><td>▇█▅▅▆▅▅▃▄▁▄▂▄▅▁▂▃▂▁▃▂▂▁▃▃▂▂▄▃▂▁▂▂▂▄▃▂▂▂▃</td></tr><tr><td>val_acc</td><td>▁█</td></tr><tr><td>val_acc_0</td><td>█▁</td></tr><tr><td>val_acc_1</td><td>▁█</td></tr><tr><td>val_acc_2</td><td>█▁</td></tr><tr><td>val_acc_3</td><td>▁█</td></tr><tr><td>val_acc_4</td><td>█▁</td></tr><tr><td>val_f1</td><td>▁█</td></tr><tr><td>val_f1_0</td><td>▁█</td></tr><tr><td>val_f1_1</td><td>▁█</td></tr><tr><td>val_f1_2</td><td>▁█</td></tr><tr><td>val_f1_3</td><td>▁█</td></tr><tr><td>val_f1_4</td><td>▁█</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_loss_batch</td><td>█▁</td></tr><tr><td>val_prec</td><td>▁█</td></tr><tr><td>val_rec</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.22156</td></tr><tr><td>loss_batch</td><td>0.05497</td></tr><tr><td>val_acc</td><td>0.87494</td></tr><tr><td>val_acc_0</td><td>0.84443</td></tr><tr><td>val_acc_1</td><td>0.88406</td></tr><tr><td>val_acc_2</td><td>0.85782</td></tr><tr><td>val_acc_3</td><td>0.87704</td></tr><tr><td>val_acc_4</td><td>0.8781</td></tr><tr><td>val_f1</td><td>0.87726</td></tr><tr><td>val_f1_0</td><td>0.7855</td></tr><tr><td>val_f1_1</td><td>0.92089</td></tr><tr><td>val_f1_2</td><td>0.82249</td></tr><tr><td>val_f1_3</td><td>0.83047</td></tr><tr><td>val_f1_4</td><td>0.86257</td></tr><tr><td>val_loss</td><td>0.3865</td></tr><tr><td>val_loss_batch</td><td>0.0727</td></tr><tr><td>val_prec</td><td>0.88313</td></tr><tr><td>val_rec</td><td>0.87494</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">crimson-sweep-11</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/ebdyle2n\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/ebdyle2n</a><br/>Synced 6 W&B file(s), 6 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220519_001204-ebdyle2n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8allo2vb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclassifier_dropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfrac: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tperc_warmup_steps: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsplit_size: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SWEEP HYPERPARAMETERS:\n",
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_f1'},\n",
      " 'parameters': {'batch_size': {'value': 32},\n",
      "                'classifier_dropout': {'values': [0.2, 0.3]},\n",
      "                'clip_grad': {'values': [True, False]},\n",
      "                'epochs': {'values': [1, 2]},\n",
      "                'frac': {'value': 1.0},\n",
      "                'learning_rate': {'values': [3e-05, 5e-05]},\n",
      "                'max_length': {'values': [64, 128]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'perc_warmup_steps': {'values': [0, 0.1, 0.25]},\n",
      "                'split_size': {'value': 0.2}}}\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ski/Desktop/code/breck/notebooks/wandb/run-20220519_004840-8allo2vb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/8allo2vb\" target=\"_blank\">feasible-sweep-12</a></strong> to <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/sweeps/xkytzga7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100% of PROCESSED data...\n",
      "\tShape: (444348, 6)\n",
      "Size of data before: (444348, 6)\n",
      "Size of data after: (174359, 6)\n",
      "Applying LabelEncoder() to target.\n",
      "\tReplaced columns with sequence & target columns \n",
      "\t\t['sequence', 'target']\n",
      "Splitting data into train/valid/test.\n",
      "\tStratifiying split by target values.\n",
      "\tSplit sizes: (125538, 31385, 17436)\n",
      "Creating ComplaintsDatasets for dataloaders.\n",
      "\tApplying a stratification strategy: SKLEARN.\n",
      "\t\t - Create a ComplaintsDataset: TRAIN\n",
      "\t\t\tCreating class weights to help with imbalance in WeightedRandomSampler.\n",
      "\t\t\t (Normalize them)\n",
      "\t\t - Create a ComplaintsDataset: VALID\n",
      "\t\t - Create a ComplaintsDataset: TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "\t==== Embedding Layer ====\n",
      "\n",
      "\tbert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\tbert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\tbert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\tbert.embeddings.LayerNorm.weight                              (768,)\n",
      "\tbert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "\t==== First Transformer ====\n",
      "\n",
      "\tbert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\tbert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\tbert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\tbert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\tbert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\tbert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\tbert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\tbert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\tbert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\tbert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\tbert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "\t==== Output Layer ====\n",
      "\n",
      "\tbert.pooler.dense.weight                                  (768, 768)\n",
      "\tbert.pooler.dense.bias                                        (768,)\n",
      "\tclassifier.weight                                           (5, 768)\n",
      "\tclassifier.bias                                                 (5,)\n",
      "Number of training examples: 125,568\n",
      "Number of batches: 3,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4208320826311025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WandB logs for this epoch: 3\n",
      "val_loss: 0.38150841990095047\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.340 MB of 0.341 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.996575…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>loss_batch</td><td>█▇▅▃▂▂▃▃▄▂▂▃▃▃▁▂▂▂▂▃▂▃▂▃▂▂▂▂▁▁▂▁▂▁▁▁▁▂▂▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_acc_0</td><td>▁</td></tr><tr><td>val_acc_1</td><td>▁</td></tr><tr><td>val_acc_2</td><td>▁</td></tr><tr><td>val_acc_3</td><td>▁</td></tr><tr><td>val_acc_4</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_f1_0</td><td>▁</td></tr><tr><td>val_f1_1</td><td>▁</td></tr><tr><td>val_f1_2</td><td>▁</td></tr><tr><td>val_f1_3</td><td>▁</td></tr><tr><td>val_f1_4</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_loss_batch</td><td>▁</td></tr><tr><td>val_prec</td><td>▁</td></tr><tr><td>val_rec</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.42083</td></tr><tr><td>loss_batch</td><td>0.08007</td></tr><tr><td>val_acc</td><td>0.86927</td></tr><tr><td>val_acc_0</td><td>0.85287</td></tr><tr><td>val_acc_1</td><td>0.87155</td></tr><tr><td>val_acc_2</td><td>0.85846</td></tr><tr><td>val_acc_3</td><td>0.87493</td></tr><tr><td>val_acc_4</td><td>0.88467</td></tr><tr><td>val_f1</td><td>0.87205</td></tr><tr><td>val_f1_0</td><td>0.78729</td></tr><tr><td>val_f1_1</td><td>0.91553</td></tr><tr><td>val_f1_2</td><td>0.80895</td></tr><tr><td>val_f1_3</td><td>0.82405</td></tr><tr><td>val_f1_4</td><td>0.86673</td></tr><tr><td>val_loss</td><td>0.38151</td></tr><tr><td>val_loss_batch</td><td>0.08962</td></tr><tr><td>val_prec</td><td>0.87986</td></tr><tr><td>val_rec</td><td>0.86927</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">feasible-sweep-12</strong>: <a href=\"https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/8allo2vb\" target=\"_blank\">https://wandb.ai/ski/Consumer%20Complaints%20Prediction/runs/8allo2vb</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220519_004840-8allo2vb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(SWEEP_CONFIG_RANDOM, project=\"Consumer Complaints Prediction\")\n",
    "wandb.agent(sweep_id, train_complaints, count=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
